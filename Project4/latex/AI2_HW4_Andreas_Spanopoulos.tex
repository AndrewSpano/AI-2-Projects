\documentclass[12pt]{report}

% packages used for many things
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan}
\urlstyle{same}
% package used for the enumeration
\usepackage{enumitem}
% packages used to write more symbols and text in math mode
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts} 
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{MnSymbol}
\usepackage{csquotes}
\usepackage{arydshln}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{diagbox}
\usepackage{extarrows}
\usepackage{tikz}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


% \usepackage{geometry}
%  \geometry{
%  a4paper,
%  total={170mm,257mm},
%  left=20mm,
%  top=15mm,
%  }


% for images
\usepackage{graphicx}
\graphicspath{{./images/}}

% qed black square
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}
% xor symbol
\newcommand*\xor{\oplus}

\title{Artificial Intelligence II \\ Assignment 4 Report}
\author{Andreas - Theologos Spanopoulos (sdi1700146@di.uoa.gr)}
\date{January 17, 2021}


% ----------------    START OF DOCUMENT    ------------ %
\begin{document}
\maketitle

% ------------------------         EXERCISES 1 & 2          ------------------------ %
\section*{Exercises 1 \& 2}
For these exercises, publically available pre-trained models from the python library
\href{https://www.sbert.net/index.html}{Sentence Transformer} were used. A list with
all the available models and their respective statistics, can be found
\href{https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0}
{here}.

\subsection*{Preprocessing}
From the dataset, the following fields are parsed and stored in the class \textquote{CovidArticle}:
\begin{itemize}
    \item ID
    \item Title
    \item Abstract Text, tokenized into sentences
    \item Body Text (section names and paragraphs), tokenized into sentences
\end{itemize}
For every piece of text, the following preprocessing pipeline is followed:
\begin{enumerate}
    \item Remove URLs
    \item Remove references to bibliography
    \item Remove multiple full stops (e.g. ...)
    \item Remove the \textquote{et al.} string
    \item Remove figure references
\end{enumerate}
Numbers 1, 2 and 5 were removed because they offer no use in retrieving a sentence containing them.
Numbers 3 and 4 were removed because they break sentence tokenization.
\bigskip

\noindent Every piece of text is broken down into sentences. A list containing all the sentences can be
accessed in the CovidArticle.text getter method. This format is convinient because it allows the
sentenced to be fed directly to the sentence transformers. \clearpage


\subsection*{Sentence Embedding Approaches}
The 2 models that were finally chosen, are:
\begin{enumerate}
    \item \href{https://arxiv.org/pdf/1910.01108.pdf}{stsb-distilbert-base}.
        This model was chosen because of its good performance, and its high speed in computing the
        embeddings of sentences (4000 senteces/sec on V100 GPU).
    \item \href{https://arxiv.org/pdf/1907.11692.pdf}{stsb-roberta-base}. This model was chosen as
        its the second best model regarsing the STSb performance, and its speed is acceptable
        (2300 sentences/sec on V100 GPU).
\end{enumerate}
Both models are \href{https://arxiv.org/pdf/1908.10084.pdf}{Sentence-BERT models}, which basically
means that they use \href{https://arxiv.org/pdf/1810.04805.pdf}{BERT-like} pre-trained models to
compute the sentence embeddings.
\bigskip

Now let's discuss about how the models are used to retrieve relevant text for every query.
The pipeline used is as follows:
\begin{enumerate}
    \item Filter our articles that are irrelevant to the subject query.
    \item From the articles kept, find the best one and return it.
\end{enumerate} \smallskip

\noindent\underline{Step 1} \bigskip

\noindent For every article, a \textquote{summary} is computed. This summary is the
simplest possible: it's a list consisting of sentences which are:
\begin{enumerate}
    \item The title of the article
    \item The tokenized sentences of the abstract of the article
    \item The sections of the article, each being treated as a standalone sentence.
\end{enumerate}
Then for every model, we compute the sentence embeddings for each sentence in the summary.
These embeddings are used for filtering our irrelevant articles: Articles where the highest
cosine similarity with any sentence of the summary is lower than a specified threshold,
are discarded. This helps us keep only relevant articles and therefore compute less sentence
embeddings, as it is quite a costly operation, especially on low-end hardware.
\clearpage

\noindent\underline{Step 2} \bigskip

\noindent For the articles that have \textquote{survived}, compute the sentence embeddings
for the whole text, and pick the one that has the highest cosine similarity with the query,
on any sentence. Then, consider that sentence the passage. After that, start compuring the
cosine similarity of adjacent sentences with the initial passage, and if they are above a
0.5 threshold, append them to the passage. This process repeats until either the threshold
of 0.5 is not met on both sides, or we run out of the section in which the inital passage
belonged. \smallskip

\subsection*{Comparison}
Some test queries have been hand-written for evaluation purposes. Those queries and the
corresponding articles in which their answers lie, are listed in the queries.txt file.
\bigskip

\noindent In the task of finding the correct article containing the answer to a given query,
the models performed: \bigskip
\[
    \begin{tabular}{|c|c|c|}
        \cline{1-3}
        \backslashbox{Query}{Model} & stsb-distilbert-base & stsb-roberta-base \\
        \cline{1-3}
        Query 1 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 2 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 3 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 4 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 5 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 6 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 7 & \textcolor{red}{\xmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 8 & \textcolor{green}{\cmark} & \textcolor{red}{\xmark} \\[0.6ex]
        \cline{1-3}
        Query 9 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\[0.6ex]
        \cline{1-3}
        Query 10 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Total Accuracy \% & 80\% & 80\% \\[0.6ex]
        \cline{1-3}
    \end{tabular}
\] \clearpage

\noindent Let's also take a look at the results in the task of returning also the correct
passage along with the correct article: \bigskip
\[
    \begin{tabular}{|c|c|c|}
        \cline{1-3}
        \backslashbox{Query}{Model} & stsb-distilbert-base & stsb-roberta-base \\
        \cline{1-3}
        Query 1 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 2 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 3 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 4 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 5 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 6 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 7 & \textcolor{red}{\xmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 8 & \textcolor{green}{\cmark} & \textcolor{red}{\xmark} \\[0.6ex]
        \cline{1-3}
        Query 9 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\[0.6ex]
        \cline{1-3}
        Query 10 & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Total Accuracy \% & 80\% & 80\% \\[0.6ex]
        \cline{1-3}
    \end{tabular}
\] \bigskip

\noindent The time required to compute the embeddings of all the summaries is
\begin{itemize}
    \item 15 minutes for stsb-distilbert-base
    \item 27 minutes for stsb-roberta-base
\end{itemize} \bigskip

\noindent The average time required to return the result of a query for every model is
\begin{itemize}
    \item 17.75s for stsb-distilbert-base
    \item 24.91s for stsb-roberta-base
\end{itemize}
Note that these times get faster as more queries come in, because article text embeddings
get saved in a dictionary. \clearpage


\subsection*{Error Analysis}
For an uncorrect article/passage to be returned, there are 2 possible errors that might
have occurred:
\begin{itemize}
    \item In Step 1, the correct article got filtered out.
    \item In Step 2, the correct article was kept but did not contain the sentence
        with the maximum cosine similarity.
\end{itemize}
Let's take a look in which case each of our query failures belong to. \bigskip

\noindent By un-commenting the 2 print statements in the \textquote{find\_best\_article()}
function, we can see which articles were found relevant. The others were filtered out.
Let's make a table for every model to see what is going on:

$$\text{stsb-distilbert-base}$$
\[
    \begin{tabular}{|c|c|c|}
        \cline{1-3}
        \backslashbox{Query Failures}{Failure Reason} & Step 1 Error & Step 2 Error \\
        \cline{1-3}
        Query 7 &  & \textcolor{green}{\cmark} \\[0.6ex]
        \cline{1-3}
        Query 9 & \textcolor{green}{\cmark} &  \\[0.6ex]
        \cline{1-3}
        Total & 1/2 & 1/2 \\[0.6ex]
        \cline{1-3}
    \end{tabular}
\]

$$\text{stsb-roberta-base}$$
\[
    \begin{tabular}{|c|c|c|}
        \cline{1-3}
        \backslashbox{Query Failures}{Failure Reason} & Step 1 Error & Step 2 Error \\
        \cline{1-3}
        Query 8 & \textcolor{green}{\cmark} &  \\[0.6ex]
        \cline{1-3}
        Query 9 & \textcolor{green}{\cmark} &  \\[0.6ex]
        \cline{1-3}
        Total & 2/2 & 0/2 \\[0.6ex]
        \cline{1-3}
    \end{tabular}
\] \bigskip

\noindent From this small sample, we can deduce that the \textquote{summary}
tecnhique does not work perfectly. This is expected, as the title + the
abstract + the section titles do not capture the entirety of an article. For example,
we may have an article about coronaviruses, but in the introduction there is a reference
to the Spanish Flu, and the amount of casualties it caused. Since this information is
irrelevant to the article itself and the pieces that make up the summary, it will be
filtered out. Of course, there are way better methods to create summaries, like
\href{https://huggingface.co/transformers/task_summary.html#summarization}{summarizers},
etc. These haven't been tried, as I am running low on time. One solution could be to
decrease the similarity threshold, so that more articles get included. This will slow down
significantly the running time, so it is not an option. Therefore, improving
the summary of each article should be the way to go for improving this text retrieval
mechanism. \bigskip

\noindent One other thing that could be done is to get rid of the summaries, and just
pre-compute once the sentence embeddings for every article. On the Colab GPU, this
procedure takes about 2 hours for a model like \textit{stsb-roberta-base}. Since
cosine similarity is not a very expensive operation, this would lead to the best results,
as all the articles would be taken into account, but slower (on average) as it would
compute $O(n)$ cosine similarities for every query, where $n$ is the number of articles.
\bigskip

\noindent The above idea would technically give the most accurate results. Will it?
There is a small catch. The problem lies on the selection of the \textquote{best}
sentence. The sentence with the highest cosine similarity with the query is not always
the answer to it. This problem becomes apparent when the embeddings have not been
trained on specific domain-knowledge data, for example, text containing biological terms.
The same problem would occur in the above models, had more queries been created.
Thus, apart from the issue of identifying quickly the relevant articles, one needs
to make sure that the remaining articles can be fully understood, that is, the
sentence embeddings will capture all of its semantic meaning. This can be achieved
by fine-tuning the models on the specific domain-knowledge data.

\subsection*{Conclusion}
Taking into account the above results, model \textquote{stsb-roberta-base} can be considered
as the best out of the two models, since
\begin{enumerate}
    \item It filters out the most sentences (error analysis showed that it keeps
        around 5-25 sentences per article).
    \item Even though slower, it finds the \textit{exact} answer for most queries,
        unlike \textquote{stsb-distilbert-base} which sometimes finds
        \textit{approximate} answers.
    \item Once most sentence embeddings will have been computed, 
        it will become faster than \textquote{stsb-distilbert-base}, since it
        will only have to find the cosine similarities with many less articles.
\end{enumerate} \clearpage


% ------------------------        EXERCISE 3          ------------------------ %
\section*{Exercise 3}
Even though there is an already build model for Question Answering from the
Hugging Face library
\href{https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering}{here},
I re-implemented it myself using the
\href{https://huggingface.co/transformers/model_doc/bert.html#bertmodel}{vanilla BERT model}.
Same goes for the preprocessing of the SQuAD, I preprocessed it myself as it does not make
sense to copy everything \smiley{}.

\subsection*{Preprocessing}
All the operations regarding the preprocessing of the SQuAD can be found in the
squad\_preprocessing.py file. The procedure can be described in the following steps:
\begin{enumerate}
    \item Read the json file containig the SQuAD data. Parse it into a dataframe.
    \item Preprocess the dataframe such each example gets translated to it's
        corresponding BERT input. This is achieved my tokenizing the question
        and the context, while also adding the [CLS] and [SEP] tokens. For some conrner
        case we have, we have the following rules:
        \begin{itemize}
            \item Answers that contain only part of a token, are changed in order to contain
                the whole token.
            \item BERT inputs that are longer than the maximum specified bert input length
                (hyperprameter set in the beggining), are shortened by removing the text that
                is the \textquote{most} far away from the answers. Note that this idea is
                stupid, as information about the target is \textquote{introduced} in the input.
                It was a simple solution to implement. A better idea would be to have a rolling
                window in the context, and if the original answer is inside that window, set it
                as the answer, if it is not, set the answer to \textquote{non-existing}. This
                could also work as a way to augment the dataset. But since the examples where
                the input was longer than the allowed were few, I did not get to implementing
                this solution.
        \end{itemize}
    \item Pad the BERT inputs with 0 such that all the inputs have the same length, that of
        the input with the max length. Also add the Mask IDs and the segment IDs in the
        dataframe, in order to avoid doing it later.
    \item Question that have no answer, are assigned to the class 0 ([CLS] token).
\end{enumerate} \clearpage

\subsection*{Model}
The model is pretty simple. We run the input through the BERT, and get the embeddings
of the last $n$ hidden states. In the notebook implementation, $n=4$ was used (which means
that we stack the hidden states of the layers 9-12 of the BERT output). Then, the
concatenated output is flattened, and fed into 2 linear layers (one for the start token
and one for the end token), where their output shape is the length of the BERT input.
Normally, we should use a softmax function on the output of the above layers to get
the probabilities of each token being the start token and the end token respectively.
We do not need to apply a softmax, as the CrossEntropyLoss from PyTorch does it. Some
hyperprameters of the model are:
\begin{enumerate}
    \item Learning Rate = $10^{-6}$, for Adam
    \item Regularization (weight decay) = $10^{-2}$ for the Loss Function
    \item Batch Size = 16
    \item epochs = 2
    \item Max length of BERT input = 256
\end{enumerate}

\subsection*{Performance}
The metrics used to assess the performance of the model are:
\begin{enumerate}
    \item Training Loss: 1.81, Validation Loss: 2.08 
    \item \begin{itemize}
            \item Training EM: 49.223, $\;\;\;\;$ (HasAns: 29.176, NoHasAns: 89.236)
            \item Validation EM: 47.460, $\;$     (HasAns: 7.726, NoHasAns: 87.081)
          \end{itemize}
    \item \begin{itemize}
            \item Training F1: 64.965, $\;\;\;\;$ (HasAns: 52.805, NoHasAns: 89.236)
            \item Validation F1: 51.850, $\;$     (HasAns: 16.518, NoHasAns: 87.081)
          \end{itemize}
\end{enumerate}
The model performed decently as we can see from the above scores. The graphs
of the Losses during Training, computed as an average every $\sim$
500 batches of an epoch, can be found in the Notebook. \clearpage

\subsection*{Reasons for the shortcomings of the model}
The model is clearly overfitting on the training examples that have no answer.
If we view this as a classification task (which technically it is), that is,
we have maxlen (= 256) tokens (classes) for each example, we are trying to
classify every token as \textquote{start token} or \textquote{end token}. Start
and End tokens have many range of values (usually from 10 up to 255), while questions
with no answer (which represent 1/3 of the dataset) have their start and end tokens values
at 0. Hence, the overfitting. Ways to overcome this issue are:
\begin{enumerate}
    \item Adding Dropout between the Flattening of the BERT output and the Fully Connected
        layer.
    \item Increasing the weight decay factor (\textquote{amount} of Ridge regularization).
        Currently it sits at 0.01.
\end{enumerate}
Of course, there are more reasons for why the model is not performing better, some of
those are:
\begin{itemize}
    \item Since there are severe GPU constraints, the maximum allowed length of the
        input was set to 256. BERT can actually handle inputs of length up to 512.
        The input was truncated to 256 by removing tokens far away from the actual answers.
        Therefore, some information loss may have occurred, that prevents the model from
        making actual good predictions. Exactly 12318 training examples have length
        bigger than 256, which is roughly 1/10 of the training dataset. I tried increasing
        the maxlen hyperprameter, but memory was always an issue, even with small batch sizes
        like 16.
    \item For the tokenization task, some questions where their answer is part
        of a token, but not the whole token, have \textquote{misleading} labels. This was
        not taken into account, as the model learns to predict answers only on the
        token level, and not on the character level.
\end{itemize}
Some ideas that could help improve the performance of the model and mitigate the above
problems, are:
\begin{enumerate}
    \item Increasing the maximum length of the BERT input to it's allowed 512. For
        bigger inputs, use the \textquote{sliding window} technique mentioned in page 7.
    \item Using bert-large instead of bert-base. The difference will be quite noticable,
        but training slower as well.
    \item Using a scheduler for the learning rate (Reducing On Plateau should work).
    \item Using a second BERT model, that would take as input the question and the answer
        tokens as a string of characters, and will learn to predict which characters
        are part of the actual answer, e.g. if the predicted answer it the word
        \textquote{sixth} and the ground trugh answer is \textquote{six}, then we would
        feed to the second bert the question + \textquote{s i x t h}, and make it predict
        the first 3 letters.
\end{enumerate}

\subsection*{Comparison with fine-tuned model}
The fine-tuned (on SQuAD) that was used for comparison is the
\textquote{bert-large-uncased-whole-word-masking-finetuned-squad}. On the SQuAD 2.0, it
performed:
\begin{enumerate}
    \item \begin{itemize}
            \item Training EM: 50.476, (HasAns: 74.272, NoHasAns: 2.974)
            \item Validation EM: 39.265, (HasAns: 75.641, NoHasAns: 2.994)
          \end{itemize}
    \item \begin{itemize}
            \item Training F1: 58.286, (HasAns: 85.998, NoHasAns: 2.974)
            \item Validation F1: 44.042, (HasAns: 85.209, NoHasAns: 2.994)
          \end{itemize}
\end{enumerate}
We notice that the model is not performing perfectly. This is due to the following
reasons:
\begin{enumerate}
    \item If I am not mistaken, the model was fine tuned on SQuAD 1.0, which did not
        contain questions with not answers. Hence, the low performance on questions
        with no answer (which in turns lowers the overall scores in EM and F1).
    \item The tokenizer I used to do the tokenization was from the pre-trained model
        \textquote{bert-base-uncased}, and not the one for the same pre-trained model,
        the \textquote{bert-large-uncased-whole-word-masking-finetuned-squad}.
        Therefore, some mistakes could be happening in the tokenization, which affect
        the whole meaning of a sentence (or at least it tricks it), making it
        missclassify the start and end of a question.
    \item From some question-answer examples, some tokens were removed from the context
        in order to comply with the max bert-input-length parameter with value 256.
        It is probable that some examples lose their \textquote{sense}, as some
        context being removed could make a question with answer, unanswerable, 
\end{enumerate}
Still, even though the model is not performing perfectly, it out-performed my model.
This should not take you by surprise as
\begin{enumerate}
    \item This model used bert-large instead of bert-base. The difference is quite
        noticable in any application. Bert-large may be slower as it has 24 attention
        layers instead of 12, but it's embeddings are way more accurate.
    \item I don't have a GPU \smiley{}. The fine-tuned model was probably trained
        on many GPUs, much better that the one Google Colab offers. I was not able
        to run a Grid Search or any other technique of hyperprameter searching to
        achieve better performance, as every epoch needs 1-2 hours.
\end{enumerate}
With that said, the only advantage of my model compared to the fine-tuned, is speed.
Since I am using bert-base, it is almost thrice as fast during inference.

\end{document}
% ----------------    END OF DOCUMENT    ------------ %
